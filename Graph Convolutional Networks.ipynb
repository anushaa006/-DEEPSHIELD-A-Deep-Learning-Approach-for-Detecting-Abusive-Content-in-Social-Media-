{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.2946487665176392\n",
      "Epoch 1, Loss: 1.1836990118026733\n",
      "Epoch 2, Loss: 1.1209583282470703\n",
      "Epoch 3, Loss: 1.070948839187622\n",
      "Epoch 4, Loss: 1.0255149602890015\n",
      "Epoch 5, Loss: 0.9832141995429993\n",
      "Epoch 6, Loss: 0.9432388544082642\n",
      "Epoch 7, Loss: 0.9092664122581482\n",
      "Epoch 8, Loss: 0.878065288066864\n",
      "Epoch 9, Loss: 0.8413445949554443\n",
      "Epoch 10, Loss: 0.8125799894332886\n",
      "Epoch 11, Loss: 0.7798949480056763\n",
      "Epoch 12, Loss: 0.7528461217880249\n",
      "Epoch 13, Loss: 0.7315136790275574\n",
      "Epoch 14, Loss: 0.7079595923423767\n",
      "Epoch 15, Loss: 0.6795039772987366\n",
      "Epoch 16, Loss: 0.6635429859161377\n",
      "Epoch 17, Loss: 0.6363340616226196\n",
      "Epoch 18, Loss: 0.6157311797142029\n",
      "Epoch 19, Loss: 0.6028693914413452\n",
      "Epoch 20, Loss: 0.5858156681060791\n",
      "Epoch 21, Loss: 0.565686821937561\n",
      "Epoch 22, Loss: 0.5477676391601562\n",
      "Epoch 23, Loss: 0.5349727272987366\n",
      "Epoch 24, Loss: 0.5063024759292603\n",
      "Epoch 25, Loss: 0.49690353870391846\n",
      "Epoch 26, Loss: 0.48477795720100403\n",
      "Epoch 27, Loss: 0.4635465145111084\n",
      "Epoch 28, Loss: 0.45631593465805054\n",
      "Epoch 29, Loss: 0.44203007221221924\n",
      "Epoch 30, Loss: 0.4229796528816223\n",
      "Epoch 31, Loss: 0.41157805919647217\n",
      "Epoch 32, Loss: 0.3977735638618469\n",
      "Epoch 33, Loss: 0.3934992551803589\n",
      "Epoch 34, Loss: 0.3816644549369812\n",
      "Epoch 35, Loss: 0.3699468672275543\n",
      "Epoch 36, Loss: 0.35473087430000305\n",
      "Epoch 37, Loss: 0.3485059440135956\n",
      "Epoch 38, Loss: 0.33996230363845825\n",
      "Epoch 39, Loss: 0.3228435218334198\n",
      "Epoch 40, Loss: 0.3203475773334503\n",
      "Epoch 41, Loss: 0.3176780641078949\n",
      "Epoch 42, Loss: 0.3051561713218689\n",
      "Epoch 43, Loss: 0.29252102971076965\n",
      "Epoch 44, Loss: 0.28930121660232544\n",
      "Epoch 45, Loss: 0.2807752788066864\n",
      "Epoch 46, Loss: 0.2750774323940277\n",
      "Epoch 47, Loss: 0.2713395059108734\n",
      "Epoch 48, Loss: 0.2641243040561676\n",
      "Epoch 49, Loss: 0.2574280798435211\n",
      "Epoch 50, Loss: 0.25657913088798523\n",
      "Epoch 51, Loss: 0.2495417594909668\n",
      "Epoch 52, Loss: 0.23825399577617645\n",
      "Epoch 53, Loss: 0.23337942361831665\n",
      "Epoch 54, Loss: 0.22695386409759521\n",
      "Epoch 55, Loss: 0.2247215211391449\n",
      "Epoch 56, Loss: 0.22526292502880096\n",
      "Epoch 57, Loss: 0.21587814390659332\n",
      "Epoch 58, Loss: 0.208025723695755\n",
      "Epoch 59, Loss: 0.20981216430664062\n",
      "Epoch 60, Loss: 0.20298057794570923\n",
      "Epoch 61, Loss: 0.19889776408672333\n",
      "Epoch 62, Loss: 0.1941862404346466\n",
      "Epoch 63, Loss: 0.19498352706432343\n",
      "Epoch 64, Loss: 0.18663576245307922\n",
      "Epoch 65, Loss: 0.18276169896125793\n",
      "Epoch 66, Loss: 0.18044069409370422\n",
      "Epoch 67, Loss: 0.17756694555282593\n",
      "Epoch 68, Loss: 0.17325207591056824\n",
      "Epoch 69, Loss: 0.17117281258106232\n",
      "Epoch 70, Loss: 0.1692705899477005\n",
      "Epoch 71, Loss: 0.16625981032848358\n",
      "Epoch 72, Loss: 0.1616281270980835\n",
      "Epoch 73, Loss: 0.15840156376361847\n",
      "Epoch 74, Loss: 0.15811987221240997\n",
      "Epoch 75, Loss: 0.15938860177993774\n",
      "Epoch 76, Loss: 0.15290643274784088\n",
      "Epoch 77, Loss: 0.1496219038963318\n",
      "Epoch 78, Loss: 0.15322065353393555\n",
      "Epoch 79, Loss: 0.1486133188009262\n",
      "Epoch 80, Loss: 0.14388711750507355\n",
      "Epoch 81, Loss: 0.14476840198040009\n",
      "Epoch 82, Loss: 0.14194992184638977\n",
      "Epoch 83, Loss: 0.13945241272449493\n",
      "Epoch 84, Loss: 0.13702574372291565\n",
      "Epoch 85, Loss: 0.13357098400592804\n",
      "Epoch 86, Loss: 0.13510197401046753\n",
      "Epoch 87, Loss: 0.13102388381958008\n",
      "Epoch 88, Loss: 0.12923409044742584\n",
      "Epoch 89, Loss: 0.12856483459472656\n",
      "Epoch 90, Loss: 0.12689709663391113\n",
      "Epoch 91, Loss: 0.12731529772281647\n",
      "Epoch 92, Loss: 0.12498673796653748\n",
      "Epoch 93, Loss: 0.12303446978330612\n",
      "Epoch 94, Loss: 0.11943630874156952\n",
      "Epoch 95, Loss: 0.12441112846136093\n",
      "Epoch 96, Loss: 0.11813700944185257\n",
      "Epoch 97, Loss: 0.1171286404132843\n",
      "Epoch 98, Loss: 0.11732915043830872\n",
      "Epoch 99, Loss: 0.11583865433931351\n",
      "Epoch 100, Loss: 0.11392045766115189\n",
      "Epoch 101, Loss: 0.1124008446931839\n",
      "Epoch 102, Loss: 0.1125657930970192\n",
      "Epoch 103, Loss: 0.11331797391176224\n",
      "Epoch 104, Loss: 0.11121582984924316\n",
      "Epoch 105, Loss: 0.1112150326371193\n",
      "Epoch 106, Loss: 0.10540597885847092\n",
      "Epoch 107, Loss: 0.10697987675666809\n",
      "Epoch 108, Loss: 0.10655387490987778\n",
      "Epoch 109, Loss: 0.10488981753587723\n",
      "Epoch 110, Loss: 0.10685773193836212\n",
      "Epoch 111, Loss: 0.1036803349852562\n",
      "Epoch 112, Loss: 0.1014031395316124\n",
      "Epoch 113, Loss: 0.10157355666160583\n",
      "Epoch 114, Loss: 0.09993235766887665\n",
      "Epoch 115, Loss: 0.0989634245634079\n",
      "Epoch 116, Loss: 0.09891155362129211\n",
      "Epoch 117, Loss: 0.09609788656234741\n",
      "Epoch 118, Loss: 0.09693542122840881\n",
      "Epoch 119, Loss: 0.099423348903656\n",
      "Epoch 120, Loss: 0.09433651715517044\n",
      "Epoch 121, Loss: 0.09562363475561142\n",
      "Epoch 122, Loss: 0.09697628766298294\n",
      "Epoch 123, Loss: 0.09411194920539856\n",
      "Epoch 124, Loss: 0.09408680349588394\n",
      "Epoch 125, Loss: 0.09246630221605301\n",
      "Epoch 126, Loss: 0.09138882905244827\n",
      "Epoch 127, Loss: 0.09373898804187775\n",
      "Epoch 128, Loss: 0.09098616987466812\n",
      "Epoch 129, Loss: 0.0899292603135109\n",
      "Epoch 130, Loss: 0.09186463803052902\n",
      "Epoch 131, Loss: 0.08800662308931351\n",
      "Epoch 132, Loss: 0.08960440754890442\n",
      "Epoch 133, Loss: 0.08905692398548126\n",
      "Epoch 134, Loss: 0.08644100278615952\n",
      "Epoch 135, Loss: 0.08778630942106247\n",
      "Epoch 136, Loss: 0.08790689706802368\n",
      "Epoch 137, Loss: 0.08578357845544815\n",
      "Epoch 138, Loss: 0.0830678939819336\n",
      "Epoch 139, Loss: 0.08499719947576523\n",
      "Epoch 140, Loss: 0.08503999561071396\n",
      "Epoch 141, Loss: 0.08582818508148193\n",
      "Epoch 142, Loss: 0.08252570778131485\n",
      "Epoch 143, Loss: 0.0844024047255516\n",
      "Epoch 144, Loss: 0.08133769035339355\n",
      "Epoch 145, Loss: 0.0842972844839096\n",
      "Epoch 146, Loss: 0.08141187578439713\n",
      "Epoch 147, Loss: 0.08164215832948685\n",
      "Epoch 148, Loss: 0.08106740564107895\n",
      "Epoch 149, Loss: 0.08121318370103836\n",
      "Epoch 150, Loss: 0.07865148037672043\n",
      "Epoch 151, Loss: 0.08024414628744125\n",
      "Epoch 152, Loss: 0.0790996253490448\n",
      "Epoch 153, Loss: 0.07938452810049057\n",
      "Epoch 154, Loss: 0.07843942195177078\n",
      "Epoch 155, Loss: 0.07566215097904205\n",
      "Epoch 156, Loss: 0.07869625836610794\n",
      "Epoch 157, Loss: 0.07785449922084808\n",
      "Epoch 158, Loss: 0.07615832984447479\n",
      "Epoch 159, Loss: 0.0753011405467987\n",
      "Epoch 160, Loss: 0.07577084004878998\n",
      "Epoch 161, Loss: 0.07639497518539429\n",
      "Epoch 162, Loss: 0.07648538053035736\n",
      "Epoch 163, Loss: 0.07476302981376648\n",
      "Epoch 164, Loss: 0.0749761313199997\n",
      "Epoch 165, Loss: 0.07308557629585266\n",
      "Epoch 166, Loss: 0.0737636387348175\n",
      "Epoch 167, Loss: 0.07480840384960175\n",
      "Epoch 168, Loss: 0.07243573665618896\n",
      "Epoch 169, Loss: 0.07282926887273788\n",
      "Epoch 170, Loss: 0.0730772465467453\n",
      "Epoch 171, Loss: 0.07344987988471985\n",
      "Epoch 172, Loss: 0.07177377492189407\n",
      "Epoch 173, Loss: 0.07117105275392532\n",
      "Epoch 174, Loss: 0.0716065913438797\n",
      "Epoch 175, Loss: 0.07154688984155655\n",
      "Epoch 176, Loss: 0.06937279552221298\n",
      "Epoch 177, Loss: 0.07190634310245514\n",
      "Epoch 178, Loss: 0.07029931992292404\n",
      "Epoch 179, Loss: 0.0686752200126648\n",
      "Epoch 180, Loss: 0.07058979570865631\n",
      "Epoch 181, Loss: 0.068660669028759\n",
      "Epoch 182, Loss: 0.07122993469238281\n",
      "Epoch 183, Loss: 0.06921887397766113\n",
      "Epoch 184, Loss: 0.06813997775316238\n",
      "Epoch 185, Loss: 0.06742193549871445\n",
      "Epoch 186, Loss: 0.06830888241529465\n",
      "Epoch 187, Loss: 0.06692564487457275\n",
      "Epoch 188, Loss: 0.0682164803147316\n",
      "Epoch 189, Loss: 0.0682699978351593\n",
      "Epoch 190, Loss: 0.0673239603638649\n",
      "Epoch 191, Loss: 0.06775568425655365\n",
      "Epoch 192, Loss: 0.06474348902702332\n",
      "Epoch 193, Loss: 0.0667346939444542\n",
      "Epoch 194, Loss: 0.06594350188970566\n",
      "Epoch 195, Loss: 0.06616126000881195\n",
      "Epoch 196, Loss: 0.06490114331245422\n",
      "Epoch 197, Loss: 0.06438738107681274\n",
      "Epoch 198, Loss: 0.06642336398363113\n",
      "Epoch 199, Loss: 0.06316262483596802\n",
      "Accuracy: 0.9868458217326394\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('C:/Users/gangi/OneDrive/Desktop/DL/HateSpeechData.csv')\n",
    "\n",
    "def simple_preprocess_tweet(tweet):\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    tweet = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tokens = tweet.split()\n",
    "    stop_words = set([\"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"if\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "data['cleaned_tweet'] = data['tweet'].apply(simple_preprocess_tweet)\n",
    "\n",
    "# Feature extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(data['cleaned_tweet'])\n",
    "\n",
    "# Compute cosine similarity between tweets\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "threshold = 0.5\n",
    "edges = np.argwhere(cosine_sim > threshold)\n",
    "edges = edges[edges[:, 0] != edges[:, 1]]\n",
    "\n",
    "x = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float)\n",
    "y = torch.tensor(data['class'].values, dtype=torch.long)\n",
    "\n",
    "num_nodes = x.shape[0]\n",
    "adj = torch.zeros((num_nodes, num_nodes), dtype=torch.float)\n",
    "for edge in edges:\n",
    "    adj[edge[0], edge[1]] = 1\n",
    "    adj[edge[1], edge[0]] = 1\n",
    "adj += torch.eye(num_nodes)\n",
    "\n",
    "class GraphConvLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = self.linear(x)\n",
    "        x = torch.mm(adj, x)\n",
    "        return x\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GraphConvLayer(input_dim, hidden_dim)\n",
    "        self.conv2 = GraphConvLayer(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.conv1(x, adj))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "model = GCN(input_dim=x.shape[1], hidden_dim=16, output_dim=3)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, adj)\n",
    "    loss = F.nll_loss(out, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "for epoch in range(200):\n",
    "    loss = train()\n",
    "    print(f'Epoch {epoch}, Loss: {loss}')\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    _, pred = model(x, adj).max(dim=1)\n",
    "    correct = int(pred.eq(y).sum().item())\n",
    "    acc = correct / len(y)\n",
    "    return acc\n",
    "\n",
    "accuracy = evaluate()\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
